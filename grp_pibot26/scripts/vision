#!/usr/bin/env python3

from cv_bridge import CvBridge
import pyrealsense2 as rs
import signal, time, numpy as np
import sys, cv2, rclpy, math
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
from kobuki_ros_interfaces.msg import Sound
from geometry_msgs.msg import Vector3
import numpy as np
from matplotlib import pyplot as plt


isOk= True
def signalInteruption(signum, frame):
    global isOk
    print( "\nCtrl-c pressed" )
    isOk= False

# Node processes:
def process_img(args=None):
    global isOk, color, rayon_detec, template, template2
    rclpy.init(args=args)
    rsNode= Realsense()
    
    #Déclaration des paramètres rentrés via le launch file
    rsNode.declare_parameter('color', 75)
    color = rsNode.get_parameter('color').value

    rsNode.declare_parameter('filepath', "./template/template")
    filepath = rsNode.get_parameter('filepath').value
    
    rsNode.declare_parameter('rayon_detec', 40)
    rayon_detec = rsNode.get_parameter('rayon_detec').value

    template = cv2.imread(filepath+'.jpg',0)
    template2 = cv2.imread(filepath+'2.jpg',0)

    while isOk:
        # rsNode.read_imgs()
        rsNode.analyse_imgs()
        rclpy.spin_once(rsNode, timeout_sec=0.1)


    # Stop streaming
    print("Ending...")
    rsNode.pipeline.stop()
    # Clean end
    rsNode.destroy_node()
    rclpy.shutdown()

# Realsense Node:
class Realsense(Node):
    def __init__(self, fps= 60):
        super().__init__('object_detection')
        self.image_publisher = self.create_publisher(Image, '/sensor_image', 10)        #Publisher image en couleur
        self.detec_publisher = self.create_publisher(String, '/detection_objet', 10)    #Publisher de détection de fantome
        self.depth_publisher = self.create_publisher(Image, '/sensor_depth', 10)        #Publisher image de distance
        #self.sound_publisher = self.create_publisher(Sound, '/commands/sound', 10)      #Publisher de son
        self.goal_publisher = self.create_publisher(Vector3, '/position', 10)           #Publisher position du fantome

        #ouverture du flux via la caméra
        self.pipeline = rs.pipeline()
        config = rs.config()
        config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60)
        config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60)

        align_to = rs.stream.depth
        self.align = rs.align(align_to)

        self.pipeline.start(config)

    def analyse_imgs(self):
        global color, rayon_detec, template, template2
        self.frames = self.pipeline.wait_for_frames()
        color_frame = self.frames.first(rs.stream.color)
        color_image = np.asanyarray(color_frame.get_data())


        self.bridge=CvBridge()

        #Paramètres de traitement d'image
        lo=np.array([color-20, 100, 50])
        hi=np.array([color+20, 255,255])
        kernel = np.ones((7, 7), np.uint8)
        color_info=(0, 0, 255)

        image=cv2.cvtColor(color_image, cv2.COLOR_BGR2HSV)

        #Création du masque à appliquer à l'image
        mask=cv2.inRange(image, lo, hi)
        mask=cv2.erode(mask, kernel, iterations=1)
        mask=cv2.dilate(mask, kernel, iterations=1)
        image2=cv2.bitwise_and(color_image, color_image, mask= mask)

        #Recherche de contours dans l'image à laquelle le masque a été appliqué
        elements=cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
        if len(elements) > 0:
            c=max(elements, key=cv2.contourArea)
            ((x, y), rayon)=cv2.minEnclosingCircle(c)
            if rayon>rayon_detec:   #Condition de taille sur la détection d'éléments
                fantome=False
                if(int(x)-int(rayon)>0 and int(x)+int(rayon)<848 and int(y)-int(rayon)>0 and int(y)+int(rayon)<480):  

                    #Récupération d'un échantillon carré de l'image où se situe l'objet détecté
                    echantillon = color_image[int(y)-int(rayon):int(y)+int(rayon),int(x)-int(rayon):int(x)+int(rayon),]   
                    image_gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)

                    #Chargement du template et ajustement à la taille de l'échantillon trouvé
                    down_width = 2*int(rayon)
                    down_height = 2*int(rayon)
                    down_points = (down_width, down_height)
                    template = cv2.resize(template, down_points, interpolation= cv2.INTER_LINEAR)
                    template2 = cv2.resize(template2, down_points, interpolation= cv2.INTER_LINEAR)

                    #Calcul du matching avec chacun des 2 templates
                    res = cv2.matchTemplate(image_gray, template, cv2.TM_CCOEFF_NORMED)
                    res2 = cv2.matchTemplate(image_gray, template2, cv2.TM_CCOEFF_NORMED)

                    threshold = 0.5
                    print(np.max(res))
                    print(np.max(res2))
                    fantome = np.any(res>=threshold) or np.any(res2>=threshold)

                    color_image=echantillon

                msg = String()
                msg.data = "trouve"
                #sound = Sound()
                #sound.value=0
                if(fantome): 
                    #Recherche de la position du fantome détecté par rapport au robot
                    aligned_frames =  self.align.process(self.frames)
                    #self.sound_publisher.publish(sound)
                    self.detec_publisher.publish(msg)
                    depth_frame = aligned_frames.get_depth_frame()
                    aligned_color_frame = aligned_frames.get_color_frame()
                    color_intrin = aligned_color_frame.profile.as_video_stream_profile().intrinsics
                    depth = depth_frame.get_distance(int(x), int(y))
                    
                    dx ,dy, dz = rs.rs2_deproject_pixel_to_point(color_intrin, [int(x),int(y)], depth)
                    distance = math.sqrt(((dx)**2) + ((dy)**2) + ((dz)**2))
                    goal = Vector3()
                    goal.x=dx
                    goal.y=dy
                    goal.z=dz
                    self.pose_publisher.publish(goal)

        #Publication des images en couleur dans un topic dédié
        msg_image = self.bridge.cv2_to_imgmsg(color_image,"bgr8")
        msg_image.header.stamp = self.get_clock().now().to_msg()
        msg_image.header.frame_id = "image"
        self.image_publisher.publish(msg_image)

        #Publication des images de distance dans un topic dédié
        depth_frame = self.frames.first(rs.stream.depth)
        depth_image = np.asanyarray(depth_frame.get_data())
        
        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)

        msg_depth = self.bridge.cv2_to_imgmsg(depth_colormap,"bgr8")
        msg_depth.header.stamp = msg_image.header.stamp
        msg_depth.header.frame_id = "depth"
        self.depth_publisher.publish(msg_depth)

process_img()