#!/usr/bin/env python3

from cv_bridge import CvBridge
import pyrealsense2 as rs
import signal, time, numpy as np
import sys, cv2, rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
from kobuki_ros_interfaces.msg import Sound
import numpy as np
from matplotlib import pyplot as plt


isOk= True
def signalInteruption(signum, frame):
    global isOk
    print( "\nCtrl-c pressed" )
    isOk= False

# Node processes:
def process_img(args=None):
    global isOk, color, rayon_detec, template, template2
    rclpy.init(args=args)
    rsNode= Realsense()
    
    rsNode.declare_parameter('color', 55)
    color = rsNode.get_parameter('color').value

    rsNode.declare_parameter('./template/template', 55)
    filepath = rsNode.get_parameter('filepath').value
    
    rsNode.declare_parameter('rayon_detec', 30)
    rayon_detec = rsNode.get_parameter('rayon_detec').value

    template = cv2.imread(filepath+'.jpg',0)
    template2 = cv2.imread(filepath+'2.jpg',0)

    while isOk:
        # rsNode.read_imgs()
        rsNode.analyse_imgs()
        rclpy.spin_once(rsNode, timeout_sec=0.1)


    # Stop streaming
    print("Ending...")
    rsNode.pipeline.stop()
    # Clean end
    rsNode.destroy_node()
    rclpy.shutdown()

# Realsense Node:
class Realsense(Node):
    def __init__(self, fps= 60):
        super().__init__('object_detection')
        self.image_publisher = self.create_publisher(Image, '/sensor_image', 10)
        self.detec_publisher = self.create_publisher(String, '/detection_objet', 10)
        self.depth_publisher = self.create_publisher(Image, '/sensor_depth', 10)
        self.sound_publisher = self.create_publisher(Sound, '/commands/sound', 10)
        self.pipeline = rs.pipeline()
        config = rs.config()
        config.enable_stream(rs.stream.color, 848, 480, rs.format.bgr8, 60)
        config.enable_stream(rs.stream.depth, 848, 480, rs.format.z16, 60)
        self.pipeline.start(config)

    def analyse_imgs(self):
        global color, rayon_detec, template, template2
        self.frames = self.pipeline.wait_for_frames()
        color_frame = self.frames.first(rs.stream.color)
        color_image = np.asanyarray(color_frame.get_data())

        self.bridge=CvBridge()

        lo=np.array([color-5, 100, 50])
        hi=np.array([color+5, 255,255])
        kernel = np.ones((7, 7), np.uint8)
        color_info=(0, 0, 255)

        image=cv2.cvtColor(color_image, cv2.COLOR_BGR2HSV)

        mask=cv2.inRange(image, lo, hi)
        mask=cv2.erode(mask, kernel, iterations=1)
        mask=cv2.dilate(mask, kernel, iterations=1)
        image2=cv2.bitwise_and(color_image, color_image, mask= mask)
        elements=cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]
        if len(elements) > 0:
            c=max(elements, key=cv2.contourArea)
            ((x, y), rayon)=cv2.minEnclosingCircle(c)
            if rayon>rayon_detec:
                cv2.circle(color_image, (int(x), int(y)), int(rayon), color_info, 2)
                cv2.circle(color_image, (int(x), int(y)), 1, color_info, 10)
                fantome=False
                if(int(x)-int(rayon)>0 and int(x)+int(rayon)<848 and int(y)-int(rayon)>0 and int(y)+int(rayon)<480):
                    echantillon = color_image[int(y)-int(rayon):int(y)+int(rayon),int(x)-int(rayon):int(x)+int(rayon),]
                    image_gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)

                    down_width = 2*int(rayon)
                    down_height = 2*int(rayon)
                    down_points = (down_width, down_height)
                    template = cv2.resize(template, down_points, interpolation= cv2.INTER_LINEAR)
                    template2 = cv2.resize(template2, down_points, interpolation= cv2.INTER_LINEAR)
                    res = cv2.matchTemplate(image_gray, template, cv2.TM_CCOEFF_NORMED)
                    res2 = cv2.matchTemplate(image_gray, template2, cv2.TM_CCOEFF_NORMED)

                    threshold = 0.45
                    print(np.max(res))
                    print(np.max(res2))
                    fantome = np.any(res>=threshold) or np.any(res2>=threshold)

                    color_image=echantillon

                msg = String()
                msg.data = "trouve"
                sound = Sound()
                sound.value=4
                if(fantome): 
                    self.sound_publisher.publish(sound)
                    self.detec_publisher.publish(msg)

        msg_image = self.bridge.cv2_to_imgmsg(color_image,"bgr8")
        msg_image.header.stamp = self.get_clock().now().to_msg()
        msg_image.header.frame_id = "image"
        self.image_publisher.publish(msg_image)

        depth_frame = self.frames.first(rs.stream.depth)
        depth_image = np.asanyarray(depth_frame.get_data())
        
        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)

        msg_depth = self.bridge.cv2_to_imgmsg(depth_colormap,"bgr8")
        msg_depth.header.stamp = msg_image.header.stamp
        msg_depth.header.frame_id = "depth"
        self.depth_publisher.publish(msg_depth)

process_img()